## Topic and Detail
### 4.1 토픽과 파티션
토픽은 카프카 자체다. 토픽을 만들며 시작하고 토픽을 삭제하면 데이터가 삭제되고 파이프라인이 중단된다. 그만큼 토픽은 중요한 역할을 한다.

#### 적정 파티션 개수
파티션 개수는 카프카 성능과 관련이 있다. 그렇기 때문에 토픽을 운영함에 있어 적절한 파티션 개수를 설정하고 운영하는 것이 중요하다. 고려할 점은 아래와 같다.
1. 데이터 처리량
2. 메시지 키 사용 여부
3. 브로커, 컨슈머 영향도

파티션은 카프카 병렬처리의 핵심이다. 파티션 개수가 많아지면 1:1 매핑되는 컨슈머가 들기 때문이다. 그렇기에 토픽의 파티션 개수는 데이터 처리량을 측정해서 정하는 것이 합리적이다. 
데이터 처리 속도를 올리는 방법은 1. 컨슈머 처리량을 늘리거나 2. 컨슈머를 추가해서 병렬 처리량을 늘리는 것이다.
반면 파티션 개수를 늘리고 파티션 개수만큼 컨슈머를 추가하는 방법은 데이터 처리량을 늘리는 가장 확실한 방법이다 .

> 프로듀서 전송 데이터량 < 컨슈머 데이터 처리량 * 파티션 개수

파티션 개수만큼 컨슈머 쓰레드를 운영하면 병렬처리를 극대화할 수 있다. 만약 전체 컨슈머 데이터 처리량이 프로듀서가 보내는 데이터보다 적다면 컨슈머 랙이 생기고 데이터 처리 지연이 발생한다.(이는 로직 시간 복잡도를 줄이는 것이 중요하단 소리가 된다.)
물론 파티션 개수를 무조건 늘리는게 능사는 아니다. 파티션 개수를 늘리면 컨슈머, 브로커 부담이 있기 떄문이다. 그렇기 때문에 데이터를 처리함에 있어서 지연 발생에 따른
서비스 영향도를 같이 고려하여 파티션 개수를 구하는 것이 중요하다. 

다음으로 메시지 키 사용 여부를 정하는 것이다. 메시지키 사용 여부는 데이터 처리 순서와 밀접한 연관이 있다. 프로듀서가 기본 파티셔너를 사용할 경우 메시지 키를 사용하면
프로듀서가 토픽으로 데이터를 보낼 때 메시지 키를 해시 변환해서 메시지를 파티션에 매칭시킨다. 만약 파티션 개수가 달라지면 이미 매칭된 파티션과 메시지 키의
매칭이 깨지고 파티션-메시지 키 매칭이 깨진다. 이러면 특정 메시지 키의 순서를 보장받기가 어려워진다. 

메시지 키를 사용하고 컨슈머에서 순서보장이 필요하다면 최대한 파티션 변화를 주지 말아야 한다. 만약에 파티션 변경이 불가피하다면
기존 메시지 키의 매칭을 그대로 가져가기 위해서 파티셔너를 커스텀해야 한다. 이런 문제 때문에 키 별로 처리 순서를 보장하기 위해서는
프로듀서가 전송하는 데이터 양보다 파티션 개수를 넉넉하게 잡는 것이 좋다. 순서 보장이 필요 없다면 그냥 데이터 양에 맞춰서 줄이고 늘리면 된다.

마지막으로 고려할 점은 브로커와 컨슈머의 영향도다. 카프카에서 파티션은 각 브로커의 파일 시스템을 사용한다. 그렇기에 파티션이 늘면
브로커에서 접근하는 파일 개수가 늘어난다. OS는 프로세스 당 열 수 있는 파일 최대 개수를 제한하고 있다. 그러므로 카프카 브로커가
접근하는 파일 개수를 안정적으로 유지하기 위해서 브로커당 파티션 개수를 모니터링 해야한다. 파티션 개수가 너무 많으면 분산도 답이다.

#### 토픽 정리 정책(cleanup.policy)
토픽 데이터는 시간 또는 용량에 따라 삭제 규칙을 적용할 수 있다. 또는 삭제를 원하지 않는다면 카프카 클러스와 토픽의 데이터 생명주기를 같이 할 수도 있다.
데이터가 남아 있으면 필요할 때 오프셋을 지정해서 일주일, 한 달 이상이 지난 데이터도 가져올 수 있다.

그러나 운영 비용에 문제가 생길 수도 있다. 따라서 데이터를 더 이상 사용하지 않을 경우에는 `cleanup.policy`로 데이터를 삭제할 수 있는데 옵션은 2가지다. 첫 번째는 
delete로 완전 삭제, 두 번쨰는 compact로 동일 메시지 키의 가장 오래된 데이터를 삭제하는 방법이다.

1. delete policy
토픽의 데이터를 삭제할 때는 세그먼트 단위로 삭제를 진행한다. 세그먼트는 토픽의 데이터를 저장하는 명시적인 파일 시스템 단위다. 세그먼트는 파티션마다 별개로 생성되며 세그먼트의
파일 이름은 오프셋 중 가장 작은 값이 된다. 세그먼트는 여러 조각으로 나뉘는데 segment.bytes 옵션으로 1개의 세그먼트 크기를 정할 수 있다. segment.bytes 크기보다
커질 경우에는 기존에 적재하던 세그먼트 파일을 닫고 새로운 세그먼트를 열어서 데이터를 저장한다. 데이터를 저장하기 위해 사용 중인 세그먼트를 액티브 세그먼트라고 한다.
`retention.ms` 토픽의 데이터 유지 기간을 밀리 초로 설정할 수 있는 키 값이다. `retention.bytes`는 토픽의 최대 사이즈를 정의하는 키 값이다.

2. compact policy
압축이란 메시지 키별로 해당 메시지 키의 레코드 중 오래된 데이터를 삭제하는 정책을 의미한다. 메시지 키를 기준으로 오래된 데이터를 삭제하기 때문에 삭제 정책과 다르게
1개 파티션에서 오프셋 증가가 일정하지 않을 수 있다. 

특히 토픽 압축 정책은 카프카 스트림즈의 KTable과 같이 메시지 키를 기반으로 데이터를 처리할 경우 유용하게 작용할 수 있다. 데이터의 흐름이 아닌 가장 마지막으로 
업데이트된 메시지 키의 데이터가 중요할 경우 최신 데이터 이외에는 다 삭제할 수 있기 떄문이다. 

|  오프셋  |  0   |  1  |  2  |  3  | 4 |  5  |  6  |
|:-----:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|
|  Key  |  K1  | K2  | K3  | K3  |K1   | K2  | K1  |
| Value |  V1  | V1     |V1      | V2  |V2   | V2    | V3  |

⬇

|  오프셋  |  0  |  1  |  2  |
|:-----:|:---:|:---:|:---:|
|  Key  | K3  | K2  | K1  |
| Value | V2  | V2  | V1  |

압축 정책은 액티브 세그먼트를 제ㅚ한 나머지 세그먼트들에 한해서만 데이터를 처리한다. `min.cleanable.dirty.ratio`
으로 압축 시점을 가른다. 이 옵션은 액티브 세그먼트를 제외한 세그먼트에 남아 있는 데이터의 tail 영역 레코드 수와 head 영역의 레코드 개수 비율을 뜻한다. 테일 영역의
레코드들은 `clean log`라고 부르고 압축이 완료됐기 때문에 tail에는 중복된 메시지 키가 없다. head는 압축 전 레코드들이 있으므로 중복된 메시지 키를 가진 레코드들이 있다. 

|  오프셋  |  0   |  1  |  2  |  3  | 4 |  5  |  6  |
|:-----:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|
|  Key  |  K1  | K2  | K3  | K3  |K1   | K2  | K1  |
| Value |  V1  | V1     |V1      | V2  |V2   | V2    | V3  |

>
> 0 ~ 2 클린 레코드 (3)
> 3 ~ 5 더티 레코드 (3)
> 6 액티브 세그먼트
> 
> dirty / (clean + dirty)
> = 0.5
> 
`min.cleanable.dirty.ratio`가 너무 크면 압축을 한 번에 수행하지만 용량에 부담이 될 거고 
반대라면 너무 자주 일어나서 CPU에 부담이 될 것이다. 


#### ISR(In-Sync-Replicas)
ISR은 리더 파티션과 팔로워 파티션이 모두 싱크가 된 상태를 의미한다. 복제 개수가 2인 토픽을 보자. 이 토픽에는
리더 1, 팔로워 1이 존재할거다. 자 팔로워가 리더에 싱크를 맞추는데 시간 차가 존재한다. 그럼 결국 리더, 팔로워 간 오프셋 차가 발생한다. 
이런 차이를 모니터링하기 위해서 리더 파티션은 `replica.lag.time.max.ms` 값만큼 주기를 가지고 팔로워 파티션이 데이터를 복제하는지 확인한다.
만약 팔로워 파티션이 해당 값을 넘을 때까지 데이터를 가져가지 않으면 문제가 생긴것으로 판단, `ISR 그룹`에서 제외한다. (redis centinel의 정족수 계산이랑 비슷한 느낌 같다.)

ISR로 묶인 리더, 팔로워는 파티션에 존재하는 데이터가 모두 동일하기에 팔로워는 리더로 승격될 수도 있다. 반면, ISR에서 퇴출되면 리더가 될 수 없다.
데이터 유실이 상관 없다면 ISR이 아닌 파티션을 리더로 선출할 수도 있다. `unclean.leader.election.enable`을 `true`로 두면 된다. 
`false`로 두면 이 경우에는 리더 파티션이 존재하는 브로커가 다시 시작되기까지 기다린다. 곧 서비스 중단을 의미한다. `true`로 두면 동기화되지 않은 데이터가 유실될 수 있다. 
대신 서비스 중단은 발생하지 않는다.

이 옵션은 정책적으로 무중단 운영이 중요한지 아니면 데이터가 유실이 중요한지에 따라 달라질 것이다. false로 두면 이슈가 발생한 브로커가 다시 실행될 때까지 기다려야 하므로
중단이 생길 수 있다. 이 옵션은 토픽별로 설정할 수 있으며
```shell
/bin/kafka-topics.sh \
--create \
--bootstrap-server localhost:9092 \
--topic test-topic \
--config unclean.leader.election.enable=false
```

### 프로듀서

#### acks 옵션 
acks는 0,1,all(-1)을 가질 수 있다. 이 옵션으로 프로듀서가 전송한 데이터가 카프카 클러스터에 얼마나 신뢰성 높게 저장할지 지정할 수 있다. 그리고 acks 옵션이 따라
성능이 달라질 수도 있다. 복제 개수가 1이면 크게 다른점이 없기에 복제 개수가 2일 때를 기준으로 살펴보자.

##### acks=0
acks를 0으로 설정하면 프로듀서가 리더 파티션으로 데이터를 전송했을 때 리더 파티션으로 데이터가 저장됐는지 확인하지 않는다는 뜻이다. 리더 파티션은 데이터가 저장된 
이후에 데이터가 몇 번쨰 오프셋에 저장됐는지 리턴하는데 0으로 설정되어 있으면 프로듀서는 리더 파티션에 데이터가 저장됐는지 관심이 없다. 이는 나중에 데이터가 몇 번쨰 
오프셋에 저장됐는지 확인할 수 없는 상황을 만들 수도 있다.

전송 실패시 retry를 하도록 옵션을 설정할 수 있는데 acks가 0이면 성공 여부를 알 길이 없어서 retries 옵션을 무시하는 결과를 낳는다. 물론 acks가 0일 때 네트워크
오류, 브로커 이슈 등이 발생해도 신경쓰지 않는다. 대신 1, -1보다 훨씬 빠르다.

##### acks=1
리더 파티션에만 정상적으로 적재됐는지 확인한다. 만약 리더가 제대로 받지 못했다면 받을 때까지 retry 할 수도 있다. 리더에 적재됐더라도 
데이터 유실이 있을 수도 있는데 복제 개수를 2 이상으로 하면 팔로워가 동기화를 하지 못했을 수도 있다. 이 경우 리더가 장애에 빠지면 데이터 유실로 직결된다. 

##### acks=-1 or acks=all
all, -1은 리더, 팔로워 모두를 체크한다. 그러다 보니 0, 1보다 훨씬 느리다. 그럼에도 불구하고 데이터 유실을 걱정할 일이 없어진다. all로 설정한 경우 `min.insync.replicas`
옵션에 따라서 데이터 안정성이 달라진다. ISR에 포함된 파티션을 대상으로 데이터 적재를 확인하는 최소 개수를 정하는 키 값이다. `min.insync.replicas`이 1이면
ISR 중 최소 1개 이상 파티션에 데이터가 적재됐음을 보증한다. 이 경우 acks=1과 같다. ISR에서 최초 데이터 적재는 리더에서 발생하기 때문이다.
`min.insync.replicas`를 2로 했을 때부터 의미가 있어지는 값이다. 추가적으로 `min.insync.replicas`가 복제 개수보다 작으면 문제가 된다. 같아도 장애에 빠진 경우를
고려하면 현명하지 못한 선택이 될 가능성이 매우 높다. 


#### 멱등성(idempotence) 프로듀서
멱등성은 여러 번 연산을 수행하더라도 동일한 결과를 나타내는 것을 의미한다. 이는 멱등성 프로듀서는 동일한 데이터를 여러 번 전송하더라도 카프카 클러스터에
한 번만 저장됨을 의미한다. 기본  프로듀서 동작은 `at least once delivery`이다. 적어도 한 번 전달이란 프로듀서가 클러스터에 데이터를 전송하여 저장할 때
적어도 한 번 이상 데이터를 적재할 수 있고 데이터가 유실되지 않음을 의미한다. 즉 두 번 이상 저장할 수 있다는 뜻이다.

0.11.0 이후로는 프로듀서에서 `enable.idempotence` 옵션으로 딱 한 번만 전달할 수 있도록 할 수 있다. 멱등성 프로듀서는 기본 프로듀서와 달리
데이터를 브로커로 전달할 때 프로듀서 PID와 시퀀스 번호를 함께 전달한다 그러면 브로커는 프로듀서의 PID, 시퀀스 넘버를 확인해서 동일한 메시지
적재 요청이 오면 한 번만 적재한다. 

단, 멱등성 프로듀서는 동일 세션에만 정확히 한 번 전달을 보장한다. 동일 세션이란 PID의 생명 주기를 뜻한다. 만약 멱등성 프로듀서로 동작하는
프로듀서 애플리케이션에 이슈가 생겨서 종료되고 재시작하면 PID가 달라지므로 이 경우는 한 번을 보장할 수 없다. 

`enable.idempotnece`를 true로 주면 `한 번`을 보장하기 위해서 일부 옵션들이 강제된다. 
