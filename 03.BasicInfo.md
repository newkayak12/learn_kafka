# 기본 개념

## 카프카 브로커/ 클러스터/ 주키퍼

브로커는 클라이언트와 데이터를 주고 받기 위해서 사용하는 주체이자, 데이터를 분산 저장하여 장애가 발생하더라도 안전하게 사용할 수 있도록 도와주는 애플리케이션이다.
하나의 서버에는 하나의 카프카 브로커 프로세스가 실행된다. 보통 운영에서는 3대 이상의 브로커 서버를 1개의 클러스터로 묶어서 운영한다. 클러스터로 묶인 브로커들은
프로듀서가 보낸 데이터를 안전하게 분산하고 저장하고 복제하는 역할을 수행한다.

### 데이터 저장/ 전송
프로듀서로부터 데이터를 전달받으면 카프카 브로커는 프로듀서가 요청한 토픽의 파티션에 데이터를 저장하고 컨슈머가 데이터를 요청하면 파티션에 저장한 데이터를 전달한다.
프로듀서로부터 전달된 데이터는 파일 시스템에 저장된다.

```shell
bash-5.2# ls /tmp/kafka-logs ## /config/server.properties의 log.dir 옵션에 정의한 디렉토리에 데이터를 저장한다.
                             ## 토픽 이름과 파티션 번호의 조합으로 하위 디렉토리를 생성하여 데이터를 저장한다.

__consumer_offsets-0   __consumer_offsets-17  __consumer_offsets-25  __consumer_offsets-33  __consumer_offsets-41  __consumer_offsets-5       hello.kafka-3
__consumer_offsets-1   __consumer_offsets-18  __consumer_offsets-26  __consumer_offsets-34  __consumer_offsets-42  __consumer_offsets-6       log-start-offset-checkpoint
__consumer_offsets-10  __consumer_offsets-19  __consumer_offsets-27  __consumer_offsets-35  __consumer_offsets-43  __consumer_offsets-7       meta.properties
__consumer_offsets-11  __consumer_offsets-2   __consumer_offsets-28  __consumer_offsets-36  __consumer_offsets-44  __consumer_offsets-8       recovery-point-offset-checkpoint
__consumer_offsets-12  __consumer_offsets-20  __consumer_offsets-29  __consumer_offsets-37  __consumer_offsets-45  __consumer_offsets-9       replication-offset-checkpoint
__consumer_offsets-13  __consumer_offsets-21  __consumer_offsets-3   __consumer_offsets-38  __consumer_offsets-46  cleaner-offset-checkpoint  verify-test-0
__consumer_offsets-14  __consumer_offsets-22  __consumer_offsets-30  __consumer_offsets-39  __consumer_offsets-47  hello.kafka-0
__consumer_offsets-15  __consumer_offsets-23  __consumer_offsets-31  __consumer_offsets-4   __consumer_offsets-48  hello.kafka-1
__consumer_offsets-16  __consumer_offsets-24  __consumer_offsets-32  __consumer_offsets-40  __consumer_offsets-49  hello.kafka-2

bash-5.2# ls hello.kafka-0  ##hello.kafka 토픽의 0번 파티션에 존재하는 데이터를 확인할 수 있다. 메시지와 메타데이터를 저장한다.
                            ##index는 메시지 오프셋을 인덱싱한 정보를 담는 파일이다. timeindex는 메시지에 포함된 timestamp를 기준으로 인덱싱한 정보가 담겨있다.
                            ##timestamp값은 브로커가 적재한 데이터를 삭제하거나 압축하는 데에 사용한다.

00000000000000000000.index  00000000000000000000.log  00000000000000000000.timeindex  leader-epoch-checkpoint
```

카프카는 메모리나 DB에 저장하지 않으며 따로 캐시 메모리를 구현하여 사용하지도 않는다. 일반적으로 파일 시스템으느 다루기 편하만 지속적으로 입출력할 경우 메모리에 올려서
사용하는 것보다 느리기 때문이다. 그러나 카프카는 페이지 캐시(PageCache)를 사용해서 디스크 입출력 속도를 높여서 이 문제를 해결했다. 페이지 캐시란 OS에서
파일 입출력 성능 향상을 위해서 만들어 놓은 메모리 영역을 뜻한다. 한 번 읽은 파일의 내용은 메모리의 페이지 캐시 영역에 저장시킨다. 추후 동일한 파일의 접근이
발생하면 디스크에서 읽지 않고 메모리에서 직접 읽는다. 페이지 캐시가 없다면 동작 속도는 현저히 느려졌을 것이다. 페이징을 직접 구현하고, 지속적으로 변경되는 데이터
때문에 가비지 컬렉션이 자주 일어나 속도가 느려졌을 것이다. 이러한 이유 때문에 굳이 브로커 실행에 힙을 크게 설정할 필요가 없다.


### 데이터 복제, 싱크
데이터 복제(replication)는 카프카를 장해 허용 시스템(fault tolerant system)으로 동작하도록 하는 원동력이다. 복제의 이유는 클러스터로 묶인 브로커 중 일부에
장애가 발생하더라도 데이터를 유실하지 않고 안전하게 사용하기 위함이다.

카프카의 데이터 복제는 파티션 단위로 이뤄진다. 토픽을 생성할 때 파티션의 복제 개수( replication factor )도 함께 설정되는데 직접 옵션을 선택하지 않으면
브로커에 설정된 옵션 값을 따라간다. 복제 개수의 최소 값은 1( 복제를 사용하지 않음 )이고 최댓값은 브로커 개수만큼 설정해서 사용할 수 있다.

복제된 파티션은 leaer - follower로 구성된다. 프로듀서 또는 컨슈머와 직접 통신하는 파티션을 리더라고 지칭하며, 나머지 복제 데이터를 가진 파티션을 팔로워라고 부른다.
팔로워 파티션들은 리터 파티션의 오프셋을 확인하여 현재 자신이 가진 오프셋과 차이가 나는 경우 리더 파티션으로부터 데이터를 가져와서 자신의 파티션에 저장하는데, 이를 
복제(replication)이라고 부른다. 파티션 복제로 인해 나머지 브로커에도 파티션의 데이터가 복제되므로 복제 개수만큼 저장 용량이 증가한다는 단점이 있다. 그러나
복제를 통해서 데이터를 안전하게 사용할 수 있다는 이점 때문에 카프카를 운영할 때 2개 이상의 복제 개수를 것이 중요하다.

만일 카프카 클러스터 중 0번에 장애가 생겨서 브로커가 다운되면 리더 파티션은 지위를 위임하며, 팔로워 중 하나가 이를 넘겨 받는다. 

### 컨트롤러
클러스터의 다수 브로커 중 한 대가 컨트롤러 역할을 한다. 컨트롤러는 다른 브로커들의 상태를 체크하고 브로커가 클러스터에서 빠지는 경우 해당 브로커에 존재하는 리더 파티션을
재할당한다. 카프카는 지속적으로 데이터를 처리해야 하므로 브로커의 상태가 비정상이라면 빠르게 클러스터에서 내보내는 것이 중요하다. 만약 컨트롤러가 행동 불능이 되면
다른 브로커가 위임 받는다.

### 데이터 삭제
카프카는 다른 메시징 플랫폼과 다르게 데이터를 가져가도 토픽의 데이터는 삭제되지 않는다. 또한, 컨슈머나 프로듀서가 데이터 삭제를 요청할 수도 없다. 오직 브로커만이
데이터를 삭제할 수 있다. 데이터 삭제는 파일단위로 이뤄지는데 이 단위를 `LogSegment`라고 부른다. 이 세그먼트에는 다수의 데이터가 들어 있기 때문에 일반적인
DB처럼 특정 데이터를 선별할 수는 없다. 세그먼트는 데이터가 쌓이는 동안 파일 시스템으로 열려 있으며, 카프카 브로커에  `log.segment.bytes` 또는 `log.segment.ms`
옵션이 설정되면 세그먼트 파일이 설정 값에 따라 닫힌다. 너무 작은 값으로 설정하면 열고 닫고가 잦아져서 부하가 발생할 수 있으므로 주의해야 한다. 
닫힌 세그먼트 파일은 `log.retention.bytes` 또는 `log.retention.ms` 옵션에 설정 값이 넘으면 삭제된다. 닫힌 세그먼트 파일을 체크하는 간격은
카프카 브로커의 옵션에 설정된 `log.retention.check.interval.ms`에 따른다.

### 컨슈머 오프셋 저장
컨슈머 그룹은 토픽이 특정 파티션으로부터 데이터를 가져가서 처리하고 이 파티션의 어느 레코드까지 가져갔는지 확인하기 위해서 오프셋을 커밋한다. 커밋한 오프셋은 `__consumer_offsets` 토픽에 저장한다. 
여기에 저장한 오프셋을 토대로 컨슈머 그룹은 다음 레코드를 가져가서 처리한다.

### 코디네이터(coordinator)
클러스터의 다수 브로커 중 한 대는 코디네이터 역할을 수행한다. 코디네이터는 컨슈머 그룹의 상태를 체크하고 파티션을 컨슈머와 매칭되도록 분배하는 역할을 한다. 
컨슈머가 컨슈머 그룹에서 빠지면 매칭되지 않은 파티션을 정상 동작하는 컨슈머로 할당하여 끊임 없이 데이터가 처리되도록 도와준다. 이 과정을 `rebalance`라고 부른다.


여기까지 브로커의 역할이다. 카프카 클러스터에서 주키퍼는 무슨 일을 할까? 주키퍼는 카프카의 메타 데이터를 관리하는 데에 사용된다. 주키퍼 쉘은 `zookeeper-shell.sh`로 실행할 수 있다.
```shell
bash-5.2# ./bin/zookeeper-shell.sh localhost:2181  
## 해당 명령어로 동일 환경에서 실행되는 주키퍼에 접속할 수 있다. 주키퍼 쉘을 통해서 znode를 조회하고 쉉할 수 있다.

# Connecting to localhost:2181
# Welcome to ZooKeeper!
# JLine support is disabled
#
# WATCHER::
#
# WatchedEvent state:SyncConnected type:None path:null

ls /
## root znode 하위 znode를 확인한다. 
#[admin, brokers, cluster, config, consumers, controller, controller_epoch, isr_change_notification, latest_producer_id_block, log_dir_event_notification, zookeeper]

get /brokers/ids/0
## 실습용으로 생성한 카프카 브로커에 대한 정보를 확인한다. 어떤 보안 규칙으로 통신하는지, jmx port 상태 정보, host 정보 등이 포함되어 있다. 
#{
# "listener_security_protocol_map":{
#                                   "PLAINTEXT":"PLAINTEXT"
#                                  },
# "endpoints":["PLAINTEXT://5722ad92016b:9092"],
# "jmx_port":-1,
# "host":"5722ad92016b",
# "timestamp":"1708258479515",
# "port":9092,
# "version":4
# }

get /controller
## 어느 브로커가 컨트롤러인지에 대한 정보를 가져온다.
#{
# "version":1,
# "brokerid":0,
# "timestamp":"1708258479595"
# }

ls /brokers/topics
## 카프카에 저장된 토픽들을 확인한다. `__consumer_offsets`는 카프카 내부에서 컨슈머 오프셋을 저장하기 위한 용도로 사용되는 토픽이다.
#[__consumer_offsets, hello.kafka, verify-test]

```

카프카 클러스터로 묶인 브로커들은 동일한 경로의 주키퍼 경로로 선언해야 같은 브로커 묶음이 된다. 만약 클러스터를 여러 개로 운영하면 한 개의 주키퍼에 다수의
카프카 클러스터를 연결해서 사용할 수도 있다.

## 토픽과 파티션
토픽은 카프카에서 데이터를 구분하기 위해서 사용하는 단위다. 토픽은 1개 이상의 파티션을 소유하고 있다. 파티션에는 프로듀서가 보낸 데이터들이 들어가 저장되는데
이 데이터를 `레코드(record)`라고 한다.
 
파티션은 카프카의 병렬처리의 핵심으로써 그룹으로 묶인 컨슈머들이 레코드를 병렬로 처리할 수 있도록 매칭된다. 컨슈머의 처리량의 한정된 상황에서 많은 레코드를 병렬로 처리하는 
가장 좋은 방법은 컨슈머의 개수를 늘려서 스케일아웃하는 것이다. 컨슈머를 늘리면 파티션도 늘어서 처리량이 증가하는 효과를 볼 수 있다. 파티션은 queue와 비슷하다고
생각하면 쉽다. FIFO 구조 같이 선입선출이 된다. 다만, 일반적인 자료구조로 사용되는 큐는 데이터를 가져가면 (pop) 레코드를 삭제하지만 카프카는 삭제하지 않는다.
파티션의 레코드는 컨슈머가 소비하는 것과 별개로 관리된다. 이러한 특징 떄문에 토픽 레코드는 다양한 목적을 가진 여러 컨슈머 그룹들이 토픽의 데이터를 여러 번 
가져갈 수 있다.

## 토픽 이름 제약 조건
1. 빈 문자열 토픽은 지원하지 않는다.
2. '.', '..'로 생성될 수 없다.
3. 249자 미만으로 생성되야한다.
4. 영어 대/소, 숫자, '.', ',', '_', '-'을 조합해서 생성할 수 있다.
5. `__consumer_offsets`, `__transaction_state`로는 생성할 수 없다.
6. '.', '_'가 동시에 들어가면 이슈가 발생할 수 있으므로 권장하지 않는다.
7. '.'를 '_'로, '_'를 '.'로 바꾼 경우 신규 토픽과 이름이 동일하다면 생성할 수 없다.

### 토픽 장명 예시
- `<환경>.<팀명>.<애플리케이션명>.<메시지타입>` => mrd.marketing-team.sms-platform.json
- `<프로젝트명>.<서비스명>.<환경>.<이벤트명>` => commerce.payment.prd.notification
- `<카프카 클러스터명>.<환경>.<서비스명>.<메시지타입>` => aws-kafka.live.marketing-platform.json

컨벤션을 정하고 따르지 않으면 기술 부채로 남아 훗날 문제가 될 것이다. 따라서 처음부터 컨벤션에 맞춰 잘 작성하는 것이 좋다.

## 레코드
레코드는 타임스탬프, 메시지 키, 메시지 값, 오프셋, 헤더로 구성되어 있다. 프로듀서가 생성한 레코드가 브로커로 전송되면 오프셋과 타임스탬프가 지정되어 저장된다. 
브로커에 한 번 적재된 레코드는 수정할 수 없고 로그 리텐션 기간 또는 용량에 따라서만 삭제된다.

타임 스탬프는 프로듀서에서 해당 레코드가 생성된 시점의 유닉스 타임이 설정된다. 컨슈머는 레코드의 타임스탬프를 토대로 레코드가 언제 생성되었는지 알 수 있다.
다만, 프로듀서가 레코드를 생성할 떄 임의의 타임스탬프 값을 설정할 수 있고, 토픽 설정에 따라 브로커에 적재된 시간(LogAppendTime)으로 설정될 수도 있다는 것에 유의해야 한다.

메시지 키는 메시지 값을 순서대로 처리하거나 메시지 값의 종류를 나타내기 위해서 사용한다. 메시지 키를 사용하면 프로듀서가 토픽에 레코드를 전송할 떄 메시지 키의 해시 값을 토대로 파티션을 지정하게 된다.
즉, 동일한 메시지 키라면 동일 파티션에 들어가는 것이다. 다만, 정확히 어느 파티션에 지정될지 알 수 없고 파티션 개수가 변경되면 메시지 키와 파티션 매칭이 
달라지게 되므로 주의해야 한다. 만약 메시지 키를 사용하지 않으면 프로듀서에서 레코드를 전송할 때 메시지 키를 선언하지 않으면 된다. 선언하지 않으면 `null` 설정된다.
메시지 키가 `null`로 설정된 레코드는 프로듀서 기본 설정 파티셔너에 따라서 파티션에 분배되어 적재된다.

메시지 값에는 실질적으로 처리할 데이터가 들어 있다. 메시지 키와 메시지 값은 직렬화되어 브로커로 전송되기 때문에 컨슈머가 이용할 때는 직렬화한 형태와 동일한
형태로 역직렬화를 수행해야 한다. 레코드의 오프셋은 0 이상의 숫자로 이뤄져 있다. 레코드의 오프셋은 직접 지정할 수 없고 브로커에 저장될 때 이전에 전송된 레코드의
`오프셋 + 1` 값으로 생성된다. 오프셋은 카프카 컨슈머가 데이터를 가져갈 때 사용된다. 오프셋을 사용하면 컨슈머 그룹으로 이루어진 카프카 컨슈머들이 파티션의 데이터를
어디까지 가져갔는지 명확히 지정할 수 있다.

헤더는 레코드의 추가적인 정보를 담는 메타데이터 저장소 용도로 사용한다. 헤더는 키/값으로 데이터를 추가하여 레코드의 속성을 저장하여 컨슈머에서 참조할 수 있다.

## 카프카 클라이언트
카프카 클러스터에 명령을 내리거나 데이터를 송수신하기 위해서 카프카 클라이언트 라이브러리는 카프카 프로듀서, 컨슈머, 어드민 클라이언트를 제공하는 카프카 클라이언트를 사용해서
애플리케이션을 개발한다. 


### 프로듀서
카프카 데이터의 시작점이다. 필요한 데이터를 선언하고 브로커의 특정 토픽의 파티션에 전송한다. 프로듀서는 데이터를 전송할 때 리더 파티션을 가지고 있는 
카프카 브로커와 직접 통신한다. 

[Example](./producer/src/main/java/SimpleProducer.java)
```shell
 ./bin/kafka-topics.sh \
  --bootstrap-server \
  localhost:9092 \
  --create \
  --topic test \
  --partitions 3

# [main] INFO org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values:
# 	acks = 1
# 	batch.size = 16384
# 	bootstrap.servers = [192.168.0.11:9092]
# 	buffer.memory = 33554432
# 	client.dns.lookup = default
# 	client.id = producer-1
# 	compression.type = none
# 	connections.max.idle.ms = 540000
# 	delivery.timeout.ms = 120000
# 	enable.idempotence = false
# 	interceptor.classes = []
# 	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
# 	linger.ms = 0
# 	max.block.ms = 60000
# 	max.in.flight.requests.per.connection = 5
# 	max.request.size = 1048576
# 	metadata.max.age.ms = 300000
# 	metadata.max.idle.ms = 300000
# 	metric.reporters = []
# 	metrics.num.samples = 2
# 	metrics.recording.level = INFO
# 	metrics.sample.window.ms = 30000
# 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
# 	receive.buffer.bytes = 32768
# 	reconnect.backoff.max.ms = 1000
# 	reconnect.backoff.ms = 50
# 	request.timeout.ms = 30000
# 	retries = 2147483647
# 	retry.backoff.ms = 100
# 	sasl.client.callback.handler.class = null
# 	sasl.jaas.config = null
# 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
# 	sasl.kerberos.min.time.before.relogin = 60000
# 	sasl.kerberos.service.name = null
# 	sasl.kerberos.ticket.renew.jitter = 0.05
# 	sasl.kerberos.ticket.renew.window.factor = 0.8
# 	sasl.login.callback.handler.class = null
# 	sasl.login.class = null
# 	sasl.login.refresh.buffer.seconds = 300
# 	sasl.login.refresh.min.period.seconds = 60
# 	sasl.login.refresh.window.factor = 0.8
# 	sasl.login.refresh.window.jitter = 0.05
# 	sasl.mechanism = GSSAPI
# 	security.protocol = PLAINTEXT
# 	security.providers = null
# 	send.buffer.bytes = 131072
# 	ssl.cipher.suites = null
# 	ssl.enabled.protocols = [TLSv1.2]
# 	ssl.endpoint.identification.algorithm = https
# 	ssl.key.password = null
# 	ssl.keymanager.algorithm = SunX509
# 	ssl.keystore.location = null
# 	ssl.keystore.password = null
# 	ssl.keystore.type = JKS
# 	ssl.protocol = TLSv1.2
# 	ssl.provider = null
# 	ssl.secure.random.implementation = null
# 	ssl.trustmanager.algorithm = PKIX
# 	ssl.truststore.location = null
# 	ssl.truststore.password = null
# 	ssl.truststore.type = JKS
# 	transaction.timeout.ms = 60000
# 	transactional.id = null
# 	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
#
# [main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
# [main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
# [main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1710334011117
# [kafka-producer-network-thread | producer-1] INFO org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: CihwpILxSJCziJY-haxLMg
# [main] INFO SimpleProducer - ProducerRecord(topic=test, partition=null, headers=RecordHeaders(headers = [], isReadOnly = true), key=null, value=testMsg, timestamp=null)
# // record 출력
# [main] INFO org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
#
````
정상 적재가 됐다면 토픽을 확인해보자.

```shell
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
##testMsg
```

### 프로듀서 중요 개념
프로듀서는 카프카 브로커를 데이터를 전송할 때 내부적으로 파티셔너, 배치 생성 단계를 거친다.
전송하고자 하는 데이터는 ProducerRecord 클래스를 통해서 생성했다. 더 적극적으로 파티션 번호를 지정하거나 타임스탬프, 메시지 키를 설정할 수도 있다.
레코드의 타임스탬프는 카프카 브로커에 저장될 때 브로커 시간을 기준으로 설정하지만 필요에 따라 레코드 생성 시간 또는 그 이전, 이후로 설정할 수도 있다.

KafkaProducer가 send()를 호출하면 ProducerRecord는 파티셔너(partitioner)에서 토픽이 어느 파티션으로 전송될 것인지 정해진다. KafkaProducer 인스턴스를
생성할 때 파티셔너를 지정하지 않으면 DefaultPartitioner로 설정되어 파티션이 정해진다. 파티셔너에 의해 구분된 레코드는 데이터를 전송하기 전에 accumulator에 
데이터를 버퍼로 쌓아놓고 발송한다. 이 버퍼는 배치로 묶어서 전송함으로써 카프카의 프로듀서 처리량을 향상시키는데 도움을 준다.

프로듀서 API를 사용하면 "UniformStickyPartitioner", "RoundRobinPartitioner" 2개의 파티션을 제공한다. 2.5.0에서는 파티셔너 지정이 없으면
"UniformStickyPartitioner"가 기본 설정된다. "UniformStickyPartitioner", "RoundRobinPartitioner" 둘 다 메시지 키가 있으면 메시지 키의
해시값과 파티션을 매칭하는 데이터를 전송한다는 점이 동일하다. 메시지 키가 없으면 파티션에 최대한 동하게 분배하는 로직이 있는데 "UniformStickyPartitioner"이
"RoundRobinPartitioner"의 개선판이라고 기억하면 된다.

RoundRobin은 파티션 순회가 기본이기에 배치로 묶이는 빈도가 낮아 성능 향상에 기대 폭이 좁다. 그래서 "UniformStickyPartitioner"는 accumulator에서 데이터가
배치로 모두 묶일 때까지 기다렸다 전송하므로 성능상 이점이 크다.

추가적으로 카프카 프로듀서는 압축 옵션으로 브로커로 전송 시 압축 방식을 정할 수 있는데 압축 옵션을 정하지 않으면 raw 상태로 전송된다. 압축은 gzip, snappy, lz4, zstd를 지원한다.
압축을 하면 네트워크 처리량에서 이득을 보지만 압축 해제 시 CPU, 메모리에서 손해를 보므로 적절하게 선택하는 것이 좋다.

### 프로듀서 주요 옵션

1. 필수 옵션
- bootstrap.servers : 프로듀서가 데이터를 전송할 대상 카프카 클러스터에 속한 `브로커의 호스트 이름: 포트`를 1개 이상 작성한다. 2개 이상 브로커 정보를 입력하여 일부 브로커에 이슈가 발생하더라도 접속하는 데 무리가 없도록 설정할 수 있다.
- key.serializer : 레코드에 키 직렬화 클래스를 지정한다. 
- value.serializer : 레코드의 메시지 값 직렬화 클래스를 지정한다.

2. 선택 옵션
- acks : 프로듀서가 전송한 데이터가 브로커로 정상 입력이 됐는지 확인하는데 사용하는 옵션이다. (1, 0 , -1(all)) 기본값은 1로 리더 파티션에 데이터가 저장되면 저장 성공으로 판단한다.
         0은 프로듀서가 전송한 즉시 브로커에 데이터 저장 여부와 상관 없이 성공으로 판단한다. -1 또는 all은 `min.insync.replicas` 개수에 해당하는 리더 파티션과 팔로워 파티션에
         데이터가 저장되면 성공하는 것으로 간주한다.
- buffer.memory : 브로커로 전송한 데이터를 배치로 모으기 위한 버퍼 메모리 양을 지정한다.
- retries : 프로듀서가 브로커로부터 에러를 받고 재전송 시도하는 횟수를 지정한다.
- batch.size : 배치로 전송할 레코드 최대 용량을 지정한다. 너무 작으면 자주 보내고 크면 메모리를 더 많이 사용한다.
- linger.ms : 배치 전송까지 기다리는 최소 시간이다.
- partitioner.class : 레코드를 파티션에 전송할 때 적용하는 파티셔너 클래스를 지정한다. `org.apahce.kafka.clients.producer.internals.DefualtPartitioner`
- enable.idempotence: 멱등성 프로듀서로 동작할지 설정한다.
- transactional.id : 프로듀서가 레코드를 전송할 때 레코드를 트랜잭션 단위로 묶을지 여부를 설정한다. 
